meta:
  version: "0.1.0"
  purpose: "Scoring rules and weights for coverage assessment of generated specs"

# Base scoring weights
base_weights:
  purpose_clarity: 25
  failure_mode_coverage: 20
  interface_completeness: 15
  dependency_mapping: 10
  constraint_definition: 10
  observability_planning: 10
  implementation_readiness: 10

# Component-specific weight adjustments
component_adjustments:
  storage:
    failure_mode_coverage: +10  # Storage failures are critical
    observability_planning: +5  # Need good monitoring
    
  ui:
    interface_completeness: +10  # UI interfaces are complex
    purpose_clarity: +5          # User experience clarity matters
    
  api:
    failure_mode_coverage: +15   # API reliability is critical
    constraint_definition: +10   # Performance and security constraints
    
  ai:
    failure_mode_coverage: +20   # AI safety is paramount
    constraint_definition: +15   # Cost and safety constraints

# Question value scoring
question_scoring:
  required_questions:
    purpose_statement: 25
    failure_impact: 20
    component_states: 15
    data_flow: 10
    
  optional_questions:
    accessibility_notes: 5
    performance_constraints: 8
    monitoring_requirements: 7
    
  bonus_questions:
    comprehensive_error_handling: +5
    detailed_success_criteria: +3
    proactive_monitoring: +2

# Quality multipliers (affect overall score)
quality_multipliers:
  answer_specificity:
    very_specific: 1.1      # "User logs voice note in <3s" 
    somewhat_specific: 1.0  # "Fast voice logging"
    vague: 0.8             # "Voice feature"
    
  implementation_clarity:
    very_clear: 1.1        # Specific technical details
    clear: 1.0            # General approach described
    unclear: 0.8          # Vague or contradictory
    
  consistency:
    fully_consistent: 1.0
    minor_contradictions: 0.95
    major_contradictions: 0.85

# Penalty system
penalties:
  critical_gaps:
    storage_no_backup: -15
    ui_no_error_states: -10
    api_no_auth: -20
    ai_no_safety: -25
    
  moderate_gaps:
    missing_monitoring: -5
    vague_dependencies: -3
    unclear_constraints: -4
    
  consistency_issues:
   contradictory_requirements: -8
   missing_critical_dependencies: -6
   impossible_constraints: -10
   
  completeness_issues:
    extremely_vague_purpose: -10
    no_success_criteria: -5
    missing_error_handling: -8
    undefined_interfaces: -12

# Bonus system
bonuses:
 exceptional_completeness:
   all_sections_complete: +5
   comprehensive_error_scenarios: +8
   detailed_observability_plan: +6
   proactive_security_measures: +10
   
 quality_indicators:
   measurable_success_criteria: +3
   specific_performance_targets: +4
   detailed_user_scenarios: +2
   comprehensive_testing_approach: +5
   
 innovation_points:
   creative_problem_solving: +2
   novel_approach_documented: +3
   learning_from_failures: +2

# Score interpretation bands
score_bands:
 excellent:
   range: [90, 100]
   description: "Production-ready with comprehensive coverage"
   recommendation: "Ready for implementation"
   color: "green"
   
 good:
   range: [75, 89]
   description: "Well-designed with minor gaps"
   recommendation: "Address highlighted gaps before production"
   color: "light-green"
   
 acceptable:
   range: [60, 74]
   description: "Adequate for prototype with important gaps"
   recommendation: "Suitable for MVP, plan improvements"
   color: "yellow"
   
 needs_work:
   range: [40, 59]
   description: "Significant gaps that affect quality"
   recommendation: "Address critical issues before proceeding"
   color: "orange"
   
 insufficient:
   range: [0, 39]
   description: "Major gaps that block implementation"
   recommendation: "Substantial rework needed"
   color: "red"

# Contextual scoring adjustments
context_adjustments:
 project_stage:
   prototype: 
     penalty_reduction: 0.5    # Reduce penalties by 50%
     minimum_threshold: 50     # Lower bar for prototypes
     
   mvp:
     penalty_reduction: 0.3    # Reduce penalties by 30%
     minimum_threshold: 65     # Medium bar for MVP
     
   production:
     penalty_reduction: 0.0    # Full penalties apply
     minimum_threshold: 80     # High bar for production
     
 team_experience:
   beginner:
     bonus_for_effort: +5      # Encourage thorough thinking
     
   intermediate:
     bonus_for_effort: +2
     
   expert:
     higher_standards: true    # Expect more comprehensive answers

# Dynamic scoring rules
dynamic_adjustments:
 answer_improvement_bonus:
   description: "Bonus for iterating and improving answers"
   first_attempt: 1.0
   second_attempt: 1.05
   third_attempt: 1.1
   
 session_engagement:
   description: "Bonus for thoughtful engagement vs rushing"
   thoughtful_responses: +3
   detailed_explanations: +2
   asking_clarifying_questions: +1
   
 time_spent_quality:
   description: "Balance thoroughness with efficiency"
   too_rushed: -2            # <5 minutes total
   appropriate: 0            # 8-15 minutes
   very_thorough: +2         # 15-25 minutes
   excessive: -1             # >30 minutes

# Special scoring scenarios
special_scenarios:
 ai_safety_critical:
   description: "Extra weight for AI safety questions"
   trigger: "component_type == 'ai'"
   adjustments:
     safety_guardrails: "weight *= 2"
     fallback_strategy: "weight *= 1.5"
     cost_constraints: "weight *= 1.3"
     
 public_facing_api:
   description: "Extra weight for security and reliability"
   trigger: "api_component && public_access"
   adjustments:
     authentication: "weight *= 2"
     rate_limiting: "weight *= 1.5"
     error_handling: "weight *= 1.5"
     
 user_data_handling:
   description: "Extra weight for privacy and security"
   trigger: "handles_user_data || stores_pii"
   adjustments:
     security_measures: "weight *= 1.8"
     data_retention: "weight *= 1.5"
     privacy_compliance: "weight *= 1.7"

# Score calculation formula
calculation_method: |
 1. Base Score = sum(question_weight * completion_percentage)
 2. Apply Quality Multiplier = base_score * quality_multiplier
 3. Apply Component Adjustments = adjust weights based on component type
 4. Apply Context Adjustments = adjust based on project stage, team experience
 5. Apply Penalties = subtract penalty points for gaps and issues
 6. Apply Bonuses = add bonus points for exceptional work
 7. Apply Special Scenarios = extra weight for critical areas
 8. Final Score = max(0, min(100, adjusted_score))

# Reporting configuration
reporting:
 score_display:
   show_breakdown: true
   show_improvement_suggestions: true
   show_comparison_to_typical: true
   
 gap_prioritization:
   critical_first: true
   group_by_category: true
   show_impact_assessment: true
   
 recommendation_format:
   actionable_steps: true
   estimated_effort: true
   priority_ranking: true
